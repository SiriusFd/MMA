---
title: "Exercise 4r"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(arrow)
```

## Load data

Load the following data: + applications from `app_data_sample.parquet` + edges from `edges_sample.csv`

```{r load-data}
# change to your own path!
data_path = "C:/Users/csg20/OneDrive/Desktop/Git/ORGB-672/"
app_data_sample <- read_parquet(paste0(data_path,"app_data_sample.parquet"))

app_data_sample
```

## Get gender for examiners

We'll get gender based on the first name of the examiner, which is recorded in the field `examiner_name_first`. We'll use library `gender` for that, relying on a modified version of their own [example](https://cran.r-project.org/web/packages/gender/vignettes/predicting-gender.html).

Note that there are over 2 million records in the applications table -- that's because there are many records for each examiner, as many as the number of applications that examiner worked on during this time frame. Our first step therefore is to get all *unique* names in a separate list `examiner_names`. We will then guess gender for each one and will join this table back to the original dataset. So, let's get names without repetition:

```{r gender-1}
library(gender)
#install_genderdata_package() # only run this line the first time you use the package, to get data for it

# get a list of first names without repetitions
examiner_names <- app_data_sample %>% 
  distinct(examiner_name_first)

examiner_names
```

Now let's use function `gender()` as shown in the example for the package to attach a gender and probability to each name and put the results into the table `examiner_names_gender`. Note that the first time you run this code, you need to say "Yes" in the console to download the gender data.

```{r gender-2}
# get a table of names and gender
examiner_names_gender <- examiner_names %>% 
  do(results = gender(.$examiner_name_first, method = "ssa")) %>% 
  unnest(cols = c(results), keep_empty = TRUE) %>% 
  select(
    examiner_name_first = name,
    gender,
    proportion_female
  )

examiner_names_gender
```

Finally, let's join that table back to our original applications data and discard the temporary tables we have just created to reduce clutter in our environment.

```{r gender-3}
# remove extra colums from the gender table
examiner_names_gender <- examiner_names_gender %>% 
  select(examiner_name_first, gender)

# joining gender back to the dataset
applications <- app_data_sample %>% 
  left_join(examiner_names_gender, by = "examiner_name_first")

# cleaning up
rm(examiner_names)
rm(examiner_names_gender)
gc()

```

## Guess the examiner's race

We'll now use package `wru` to estimate likely race of an examiner. Just like with gender, we'll get a list of unique names first, only now we are using surnames.

```{r race-1}
library(wru)

examiner_surnames <- applications %>% 
  select(surname = examiner_name_last) %>% 
  distinct()

examiner_surnames
```

We'll follow the instructions for the package outlined here <https://github.com/kosukeimai/wru>.

```{r race-2}
examiner_race <- predict_race(voter.file = examiner_surnames, surname.only = T) %>% 
  as_tibble()

examiner_race
```

As you can see, we get probabilities across five broad US Census categories: white, black, Hispanic, Asian and other. (Some of you may correctly point out that Hispanic is not a race category in the US Census, but these are the limitations of this package.)

Our final step here is to pick the race category that has the highest probability for each last name and then join the table back to the main applications table. See this example for comparing values across columns: <https://www.tidyverse.org/blog/2020/04/dplyr-1-0-0-rowwise/>. And this one for `case_when()` function: <https://dplyr.tidyverse.org/reference/case_when.html>.

```{r race-3}
examiner_race <- examiner_race %>% 
  mutate(max_race_p = pmax(pred.asi, pred.bla, pred.his, pred.oth, pred.whi)) %>% 
  mutate(race = case_when(
    max_race_p == pred.asi ~ "Asian",
    max_race_p == pred.bla ~ "black",
    max_race_p == pred.his ~ "Hispanic",
    max_race_p == pred.oth ~ "other",
    max_race_p == pred.whi ~ "white",
    TRUE ~ NA_character_
  ))

examiner_race
```

Let's join the data back to the applications table.

```{r race-4}
# removing extra columns
examiner_race <- examiner_race %>% 
  select(surname,race)

applications <- applications %>% 
  left_join(examiner_race, by = c("examiner_name_last" = "surname"))

rm(examiner_race)
rm(examiner_surnames)
gc()
```

## Examiner's tenure

To figure out the timespan for which we observe each examiner in the applications data, let's find the first and the last observed date for each examiner. We'll first get examiner IDs and application dates in a separate table, for ease of manipulation. We'll keep examiner ID (the field `examiner_id`), and earliest and latest dates for each application (`filing_date` and `appl_status_date` respectively). We'll use functions in package `lubridate` to work with date and time values.

```{r tenure-1}
library(lubridate) # to work with dates

examiner_dates <- applications %>% 
  select(examiner_id, filing_date, appl_status_date) 

examiner_dates
```

The dates look inconsistent in terms of formatting. Let's make them consistent. We'll create new variables `start_date` and `end_date`.

```{r tenure-2}
examiner_dates <- examiner_dates %>% 
  mutate(start_date = ymd(filing_date), end_date = as_date(dmy_hms(appl_status_date)))
```

Let's now identify the earliest and the latest date for each examiner and calculate the difference in days, which is their tenure in the organization.

```{r tenure-3}
examiner_dates <- examiner_dates %>% 
  group_by(examiner_id) %>% 
  summarise(
    earliest_date = min(start_date, na.rm = TRUE), 
    latest_date = max(end_date, na.rm = TRUE),
    tenure_days = interval(earliest_date, latest_date) %/% days(1)
    ) %>% 
  filter(year(latest_date)<2018)

examiner_dates
```

Joining back to the applications data.

```{r tenure-4}
applications <- applications %>% 
  left_join(examiner_dates, by = "examiner_id")

rm(examiner_dates)
gc()
```

## 1. Create variable for application processing time 'app_proc_time' that measures the number of days (or weeks) from application filing date, until the final decision on it (patented or abandoned)

First, find the columns I am going to use

```{r column check}
colnames(applications)
# "filing_date", "patent_issue_date", "abandon_date"
```

create the variable 'app_proc_time'

```{r app_proc_time creation}
library(dplyr)

df = applications

df <- df %>%
  # Ensure the date columns are in the Date format
  mutate(
    filing_date = as.Date(filing_date),
    patent_issue_date = as.Date(patent_issue_date),
    abandon_date = as.Date(abandon_date),
    disposal_type = as.character(disposal_type)
  ) %>%
  # First, drop rows where both patent_issue_date and abandon_date are NA
  filter(!(is.na(patent_issue_date) & is.na(abandon_date))) %>%
  # Then calculate the processing time in days based on disposal_type
  mutate(
    app_proc_time = case_when(
      disposal_type == "ISS" ~ as.numeric(patent_issue_date - filing_date),
      TRUE ~ as.numeric(abandon_date - filing_date)
    )
  ) %>%
  # Finally, refine the dropping based on disposal_type and relevant dates
  filter(!(disposal_type == "ISS" & is.na(patent_issue_date)) & !(disposal_type != "ISS" & is.na(abandon_date)))
```

```{r}
df %>% head(10)
```

realize there are some negative values (outliers)

```{r app_proc_time cleaning}
sum(df$app_proc_time < 0)

df <- df %>%
  filter(app_proc_time >= 0) %>%
  filter(!is.na(app_proc_time))
```

## 2. Use linear regression models `lm()` to estimate the relationship between centrality and `app_proc_time`

### Create advice networks from \`edges_sample\` and calculate centrality scores for examiners

```{r}
#read data 
edge = read.csv('C:/Users/csg20/OneDrive/Desktop/Git/ORGB-672/Ex3/edges_sample.csv')
```

```{r}
library(igraph)
```

```{r}
edge = drop_na(edge)
```

create networks

```{r}
g <- graph_from_data_frame(d = edge[, c("ego_examiner_id", "alter_examiner_id")], directed = TRUE)
```

Calculate centrality

```{r}
# Degree Centrality
degree_centrality <- degree(g, mode = "all")

# Betweenness Centrality
betweenness_centrality <- betweenness(g, directed = TRUE)

# Closeness Centrality
closeness_centrality <- closeness(g, mode = "all")
```

merge score back to df

```{r}
# Create a data frame of centrality measures
centrality_measures <- data.frame(
  examiner_id = V(g)$name,
  degree = degree_centrality,
  betweenness = betweenness_centrality,
  closeness = closeness_centrality
)

# Ensure examiner_id is the correct type for joining
df$examiner_id <- as.character(df$examiner_id)
centrality_measures$examiner_id <- as.character(centrality_measures$examiner_id)
```

Filter `df` to Keep Rows with `examiner_id` Present in `centrality_measures` and merge

```{r}
df <- df %>%
  filter(examiner_id %in% centrality_measures$examiner_id)

df <- merge(df, centrality_measures, by = "examiner_id")
```

### Build the model

#### Variables that could potentially affect both the centrality of examiners in the advice network and the processing time of patent applications

1.  **`gender`**: Gender may influence networking behaviors and patterns within organizations, potentially affecting both centrality and processing times.

2.  **`race`**: Similar to gender, racial dynamics within organizations can influence how individuals are positioned within informal networks and could impact processing times due to diversity in collaboration styles.

3.  **`tenure_days`**: The length of time an examiner has been with the organization could affect both their centrality in the network (with more tenured examiners potentially having more connections) and their efficiency or speed in processing applications.

4.  **`examiner_art_unit`**: Different art units may have varying average processing times due to the complexity of the applications they handle and the internal dynamics of the unit, which can also affect the centrality of examiners within and across these units.

5.  **`uspc_class`**: The technological area of the application, represented by its classification, might influence the processing time due to varying levels of complexity and the examiner's familiarity with the subject matter. These factors might also correlate with an examiner's centrality if certain areas require or facilitate more collaboration and advice sharing.

df is too big to process, thus do sampling based on examiner_id - get 5 instances for each examiner_id

```{r model building-2}
df_sampled <- df %>%
  group_by(examiner_id) %>%
  slice_head(n = 5) %>%
  ungroup()
```

there are too many categories in **`examiner_art_unit`**,**`uspc_class`**

|                                                           |                                                                                                            |
|-----------------------------------------------------------|------------------------------------------------------------------------------------------------------------|
| [1600](https://www.patentbots.com/stats/tech-center/1600) | Biotechnology and Organic Chemistry                                                                        |
| [1700](https://www.patentbots.com/stats/tech-center/1700) | Chemical and Materials Engineering                                                                         |
| [2100](https://www.patentbots.com/stats/tech-center/2100) | Computer Architecture and Software                                                                         |
| [2400](https://www.patentbots.com/stats/tech-center/2400) | Networking, Multiplexing, Cable, and Security                                                              |
| [2600](https://www.patentbots.com/stats/tech-center/2600) | Communications                                                                                             |
| [2800](https://www.patentbots.com/stats/tech-center/2800) | Semiconductors/Memory, Circuits/Measuring and Testing, Optics/Photocopying, Printing/Measuring and Testing |
| [2900](https://www.patentbots.com/stats/tech-center/2900) | Design                                                                                                     |
| [3600](https://www.patentbots.com/stats/tech-center/3600) | Transportation, Construction, Electronic Commerce, Agriculture, National Security and License and Review   |
| [3700](https://www.patentbots.com/stats/tech-center/3700) | Mechanical Engineering, Manufacturing and Medical Devices/Processes                                        |
| [3900](https://www.patentbots.com/stats/tech-center/3900) | Reexam/Abandonments                                                                                        |

based on the info shown above, change the examinor_art_unit to simplified categories

```{r examinor_art_unit engineering}
library(stringr)

df_sampled <- df_sampled %>%
  mutate(
    examiner_art_unit = as.character(examiner_art_unit),
    examiner_art_unit = str_sub(examiner_art_unit, 1, 2), # Keep the first two digits
    examiner_art_unit = paste0(examiner_art_unit, "00"), # Append '00' at the end
    examiner_art_unit = as.factor(examiner_art_unit) # Convert to factor
  )

# View the changes
head(df_sampled$examiner_art_unit)
```

for **`uspc_class`** I choose to change it based on ranges

```{r}
df_sampled <- df_sampled %>%
  mutate(
    uspc_class = as.numeric(uspc_class), # Ensure it's numeric
    uspc_class_range = cut(
      uspc_class,
      breaks = seq(0, 900, by = 100), # Define the ranges
      include.lowest = TRUE, # Include the lowest value
      labels = paste(seq(0, 800, by = 100), seq(100, 900, by = 100) - 1, sep = "-"), # Define the labels for the ranges
      right = FALSE # Make the interval left-closed [a,b)
    )
  )

df_sampled$uspc_class_range <- addNA(df_sampled$uspc_class_range, ifany = TRUE) # Converts NA to a factor level
levels(df_sampled$uspc_class_range)[is.na(levels(df_sampled$uspc_class_range))] <- "unknown"

head(df_sampled$uspc_class_range)
```

```{r model building-3}
lm_model <- lm(app_proc_time ~ degree + betweenness + closeness + gender + race + tenure_days + examiner_art_unit + uspc_class_range, data = df_sampled)
```

```{r}
summary(lm_model)
```

## 3. Does this relationship differ by examiner gender? -- Hint: Include an interaction term \`gender x centrality\` into your models

#### Experiment 1 - include degree \* gender

```{r}
lm_model_2 <- lm(app_proc_time ~ degree + betweenness + closeness + tenure_days + examiner_art_unit + uspc_class_range + degree * gender, data = df_sampled)
```

```{r}
summary(lm_model_2)
```

#### Experiment 2 - include betweeness \* gender

```{r}
lm_model_3 <- lm(app_proc_time ~ degree + betweenness + closeness + tenure_days + examiner_art_unit + uspc_class_range + betweenness * gender, data = df_sampled)
```

```{r}
summary(lm_model_3)
```

#### Experiment 3 - include closeness\* gender

```{r}
lm_model_3 <- lm(app_proc_time ~ degree + betweenness + closeness + tenure_days + examiner_art_unit + uspc_class_range + closeness* gender, data = df_sampled)
```

```{r}
summary(lm_model_3)
```

#### Experiment 4 - include all three centrality measures \* gender

```{r}
lm_model_4 <- lm(app_proc_time ~ degree + betweenness + closeness + tenure_days + examiner_art_unit + uspc_class_range + degree*gender + closeness*gender + betweenness*gender, data = df_sampled)
```

```{r}
summary(lm_model_4)
```

#### Analysis

The interaction terms (**`degree:gendermale`**, **`betweenness:gendermale`**, **`closeness:gendermale`**) in the models where they are included individually show that there is a differential effect based on gender, although the significance and magnitude of these interactions vary. When the interaction terms are included individually, they provide clear evidence that gender moderates the relationship between each centrality measure and **`app_proc_time`**.

**Evidence of Gender Differences**: The consistent significance of the interaction terms across different models provides robust evidence that the relationship between centrality and **`app_proc_time`** does indeed differ by gender. This suggests that male and female examiners may experience different benefits from their network positions in terms of processing patent applications.

## 4. Discuss your findings and their implication for the USPTO

**Tailored Network Development**: The differential impact of centrality on processing times by gender suggests that network development initiatives could be more tailored. For instance, fostering environments that encourage diverse networking styles and ensuring that all examiners, regardless of gender, can build and leverage effective networks might enhance overall operational efficiency.

**Equity in Networking Opportunities**: The findings highlight the need to ensure equitable access to networking opportunities within the USPTO. By acknowledging that the professional benefits derived from networking may vary by gender, the USPTO can take steps to create a more inclusive environment that supports diverse networking strategies and connections.

**Policy and Decision-Making**: The evidence that gender moderates the relationship between centrality and efficiency could inform policy development and strategic decision-making within the USPTO. This might include revisiting workload distribution, performance evaluation criteria, and promotion pathways to ensure they reflect an understanding of the nuanced roles that networks play in professional success.
